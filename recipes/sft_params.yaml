# Dataset configuration
dataset_name: "pir"  # Options: "medical", "pir"
dataset_path: "jonluj/pir"
split: "train"
test_size: 0.05

# Model configuration
model_name_or_path: "Qwen/Qwen2.5-0.5B-Instruct"
torch_dtype: "bfloat16"
tokenizer_name_or_path: "Qwen/Qwen2.5-0.5B-Instruct"
trust_remote_code: false
attn_implementation: "flash_attention_2"
device_map:

# LoRA configuration
peft_enabled: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
bias: "none"
target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

# Training configuration
output_dir: "./outputs"
optim: adamw_torch
learning_rate: 5.0e-6
weight_decay: 0.1
warmup_ratio: 0.1
lr_scheduler_type: "cosine"
logging_steps: 5
logging_strategy: "steps"
bf16: true
fp16: false
per_device_train_batch_size: 2
gradient_accumulation_steps: 4
num_train_epochs: 3
save_steps: 100
save_total_limit: 2
eval_strategy: "epoch"
max_grad_norm: 1.0
report_to: "wandb"
wandb_project: "qwen_reasoning"
log_on_each_node: false
packing: true
max_seq_length: 2048
seed: 42
run_prefix: "sft_pir_lora"
