# Dataset configuration
dataset_name: "pir"  # Options: "medical", "pir"
dataset_path: "jonluj/pir"
split: "train"
test_size: 0.05
shuffle: true

# Model configuration
model_name_or_path: /scratch/public_models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
torch_dtype: bfloat16
tokenizer_name_or_path: /scratch/public_models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
trust_remote_code: false
attn_implementation: "flash_attention_2"
device_map:

# LoRA configuration
peft_enabled: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
bias: "none"
target_modules: all-linear

# Training configuration
output_dir: "./outputs"
optim: adamw_torch
learning_rate: 1.0e-4
weight_decay: 0.1
warmup_ratio: 0.1
lr_scheduler_type: "cosine"
logging_steps: 5
logging_strategy: "steps"
bf16: true
fp16: false
per_device_train_batch_size: 4
gradient_accumulation_steps: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: False
num_train_epochs: 3
save_steps: 100
save_total_limit: 2
eval_strategy: "epoch"
max_grad_norm: 1.0
report_to: "wandb"
wandb_project: "qwen_reasoning"
log_on_each_node: false
packing: false
max_seq_length: 4096
seed: 42
run_prefix: "sft_pir_lora"
use_liger: true
log_level: info
